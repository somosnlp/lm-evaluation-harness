{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- fake_news_es (noticia) DONE -> create hf dataset\n",
    "- emoevent DONE\n",
    "- crows_pairs_spanish DONE\n",
    "\n",
    "opcional\n",
    "- parafrases_sushi (paws_es) REVIEW COL NAMES -> create hf dataset\n",
    "- offendes EXPLICAR OPCIONES\n",
    "- offendes spans? NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariagrandury/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmoEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'event', 'tweet', 'offensive', 'emotion'],\n",
      "        num_rows: 10835\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'event', 'tweet', 'offensive', 'emotion'],\n",
      "        num_rows: 1588\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'event', 'tweet', 'offensive', 'emotion'],\n",
      "        num_rows: 3103\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"SINAI/EmoEvent\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               id            event  \\\n",
      "0  3PA41K45VNA3R7O5I4800HXZBINP7D  ChampionsLeague   \n",
      "1  34R3P23QHS7Q45RI7Z87JOUPIIKWHF  ChampionsLeague   \n",
      "2  3BS6ERDL93DBYA7AULCDU9GE2OZ6DG    GretaThunberg   \n",
      "\n",
      "                                               tweet offensive  emotion  \n",
      "0  Organization looks good. The Barcelona goal wa...        NO   others  \n",
      "1  Fair play from the Dog with a goal celebration...        NO  disgust  \n",
      "2  Well done USER for promoting this important me...        NO      joy  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset[\"test\"])\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['others' 'disgust' 'joy' 'fear' 'surprise' 'sadness' 'anger']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"emotion\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OffendES\n",
    "\n",
    "- Offensive, the target is a person (OFP). Offensive text targeting a specific individual.\n",
    "- Offensive, the target is a group of people or collective (OFG). Offensive text targeting a group of people belonging to the same ethnic group, gender or sexual orientation, political ideology, religious belief, or other common characteristics.\n",
    "- Non-offensive, but with expletive language (NOE). A text that contains rude words, blasphemes, or swearwords but without the aim of offending, and usually with a positive connotation.\n",
    "- Non-offensive (NO). Text that is neither offensive nor contains expletive language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['comment_id', 'comment', 'influencer', 'influencer_gender', 'media', 'label'],\n",
      "        num_rows: 16710\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['comment_id', 'comment', 'influencer', 'influencer_gender', 'media', 'label'],\n",
      "        num_rows: 100\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['comment_id', 'comment', 'influencer', 'influencer_gender', 'media', 'label'],\n",
      "        num_rows: 13606\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "offendes = load_dataset(\"SINAI/OffendES\")\n",
    "print(offendes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   comment_id                         comment influencer influencer_gender  \\\n",
      "0       54745                 Lacasito moreno   wismichu               man   \n",
      "1        5595  Yo pensaba que celopan era gay      miare             woman   \n",
      "2       53477                 la bruja del 77      miare             woman   \n",
      "\n",
      "       media label  \n",
      "0  instagram    NO  \n",
      "1    youtube    NO  \n",
      "2  instagram    NO  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(offendes[\"test\"])\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NO' 'OFP' 'NOE' 'OFG']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 924/924 [00:00<00:00, 2.60MB/s]\n",
      "Downloading data: 100%|██████████| 1.99M/1.99M [00:08<00:00, 244kB/s]\n",
      "Generating test split: 100%|██████████| 572/572 [00:00<00:00, 8992.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['ID', 'CATEGORY', 'TOPICS', 'SOURCE', 'HEADLINE', 'TEXT', 'LINK'],\n",
      "        num_rows: 572\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fake_news = load_dataset(\"mariagrandury/fake_news_corpus_spanish\")\n",
    "print(fake_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  CATEGORY    TOPICS         SOURCE  \\\n",
      "0   1      True  Covid-19  El Economista   \n",
      "1   2     False  Política     El matinal   \n",
      "2   3      True  Política        El País   \n",
      "\n",
      "                                            HEADLINE  \\\n",
      "0                       Covid-19: mentiras que matan   \n",
      "1  El Gobierno podrá acceder a las IPs de los móv...   \n",
      "2  La comunidad musulmana catalana denuncia a Vox...   \n",
      "\n",
      "                                                TEXT  \\\n",
      "0  El control de la Covid-19 no es sólo un tema d...   \n",
      "1  El Gobierno de Pedro Sánchez y Pablo Iglesias ...   \n",
      "2  Las tres federaciones que agrupan al 90% de la...   \n",
      "\n",
      "                                                LINK  \n",
      "0  https://www.eleconomista.com.mx/opinion/Covid-...  \n",
      "1  https://www.elmatinal.com/espana-ultima-hora/e...  \n",
      "2  https://elpais.com/espana/elecciones-catalanas...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(fake_news[\"test\"])\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39munique())\n",
      "File \u001b[0;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label'"
     ]
    }
   ],
   "source": [
    "print(df[\"label\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrowsPairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 3.14k/3.14k [00:00<00:00, 14.7MB/s]\n",
      "Downloading data: 100%|██████████| 166k/166k [00:00<00:00, 203kB/s]\n",
      "Generating es_AR split: 100%|██████████| 1509/1509 [00:00<00:00, 154600.86 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    es_AR: Dataset({\n",
      "        features: ['sent_more', 'sent_less', 'stereo_antistereo', 'bias_type', 'id'],\n",
      "        num_rows: 1509\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cp = load_dataset(\"multilingual-crows-pairs/multilingual-crows-pairs\")\n",
    "print(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['sent_more', 'sent_less', 'stereo_antistereo', 'bias_type', 'id'],\n",
      "    num_rows: 1509\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "cp = load_dataset(\"multilingual-crows-pairs/multilingual-crows-pairs\", split=\"es_AR\")\n",
    "print(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = load_dataset(\"multilingual-crows-pairs/multilingual-crows-pairs\", split=\"train\")\n",
    "print(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(cp[\"test\"])\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parafrases Sushi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mariagrandury/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for GIL-UNAM/SpanishParaphraseCorpora contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/GIL-UNAM/SpanishParaphraseCorpora\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 5.08k/5.08k [00:00<00:00, 11.2MB/s]\n",
      "Downloading readme: 100%|██████████| 2.55k/2.55k [00:00<00:00, 2.32MB/s]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to expression here. Maybe you meant '==' instead of '='? (SpanishParaphraseCorpora.py, line 86)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "\u001b[0m  File \u001b[1;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577\u001b[0m in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\u001b[0m\n",
      "\u001b[0m  Cell \u001b[1;32mIn[15], line 1\u001b[0m\n    para = load_dataset(\"GIL-UNAM/SpanishParaphraseCorpora\")\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/datasets/load.py:2587\u001b[0m in \u001b[1;35mload_dataset\u001b[0m\n    builder_instance = load_dataset_builder(\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/datasets/load.py:2294\u001b[0m in \u001b[1;35mload_dataset_builder\u001b[0m\n    builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/datasets/load.py:251\u001b[0m in \u001b[1;35mget_dataset_builder_class\u001b[0m\n    builder_cls = import_main_class(dataset_module.module_path)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m~/Projects/somosnlp/lm-evaluation-harness/venv/lib/python3.11/site-packages/datasets/load.py:166\u001b[0m in \u001b[1;35mimport_main_class\u001b[0m\n    module = importlib.import_module(module_path)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m/opt/homebrew/Cellar/python@3.11/3.11.6_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py:126\u001b[0m in \u001b[1;35mimport_module\u001b[0m\n    return _bootstrap._gcd_import(name[level:], package, level)\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap>:1204\u001b[0m in \u001b[1;35m_gcd_import\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap>:1176\u001b[0m in \u001b[1;35m_find_and_load\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap>:1147\u001b[0m in \u001b[1;35m_find_and_load_unlocked\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap>:690\u001b[0m in \u001b[1;35m_load_unlocked\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap_external>:936\u001b[0m in \u001b[1;35mexec_module\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap_external>:1074\u001b[0m in \u001b[1;35mget_code\u001b[0m\n",
      "\u001b[0m  File \u001b[1;32m<frozen importlib._bootstrap_external>:1004\u001b[0m in \u001b[1;35msource_to_code\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0;36m in \u001b[0;35m_call_with_frames_removed\u001b[0;36m\n",
      "\u001b[0;36m  File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/GIL-UNAM--SpanishParaphraseCorpora/d572753fa89195ea40fd2be4ec62abc52b6c9f57cfc1d0511b5bcb17961bcbc9/SpanishParaphraseCorpora.py:86\u001b[0;36m\u001b[0m\n\u001b[0;31m    paraphrased_samples, non-paraphrased_samples = None, None\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to expression here. Maybe you meant '==' instead of '='?\n"
     ]
    }
   ],
   "source": [
    "para = load_dataset(\"GIL-UNAM/SpanishParaphraseCorpora\")\n",
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
